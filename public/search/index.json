
{
    
    
    
    
        
        
        
    
        
        
        
    
        
        
        
    
    "pages": [{"date":"2023-04-27","image":"","imageAlt":"","link":"https://dntiontk.github.io/posts/rss-feed-aggregation/","summary":"This post is related to my previous post about civic code. I mentioned that there are multiple RSS feeds that the city maintains, and that this data is not particularly easy to find and parse.","tags":["aggregation","civics","code","news","rss","windsor"],"text":"this post is related to my previous post about civic code. i mentioned that there are multiple rss feeds that the city maintains, and that this data is not particularly easy to find and parse.\nwe are going to create an rss feed aggregator creates a weekly summary and automatically creates a new post on this blog.\nthe process the flow we expect to use is that our aggregator fetches the current raw feed (if it exists) from its local storage. it compares the local copy to the remote copy of the feed and uses that data to summarize the changes over the past week. the summary is published as a hugo content post, the local feed state is updated, and a commit is pushed via git.\nthe aggregator as i started to explore the open data catalogue, i found something that suprised me, although in retrospect, i should have known. the open data rss feed is horribly maintained. it seems that files are uploaded frequently, but there is no metadata. basically, every item has a title, a link (usually a ytd .csv file) and a publish date.\nbefore we get to the code, i\u0026rsquo;m going to go on a little bit of a rant. city-based open data catalogues provide an incredible opportunity for a city to grow, become more resilient, and make effective and positive change. we can better understand and address the issues that affect our communities. we can make intelligent and informed decisions, and really take control of our collective destiny.\nanyway, here is the data structure of the 5th item in the feed:\n{ \u0026#34;title\u0026#34;: \u0026#34;05 grand marais.csv\u0026#34;, \u0026#34;link\u0026#34;: \u0026#34;http://opendata.citywindsor.ca/uploads/05 grand marais.csv\u0026#34;, \u0026#34;links\u0026#34;: [ \u0026#34;http://opendata.citywindsor.ca/uploads/05 grand marais.csv\u0026#34; ], \u0026#34;pubdate\u0026#34;: \u0026#34;3/23/2023 2:46:24 pm\u0026#34;, \u0026#34;pubdateparsed\u0026#34;: \u0026#34;2023-03-23t14:46:24z\u0026#34; } what is this? after doing a bit of digging, i found out that its precipitation data from the grand marais rd. precipitation gauge.\nand this is what i mean when i say that it feels like they\u0026rsquo;ve made it intentionally bad.\nthe code now let\u0026rsquo;s get to the code. we have a pretty simple program for the most part. all the code can be found at https://github.com/dntiontk/rss-feed-aggregator.\nfirst we parse the local copy of the open data feed. if it doesn\u0026rsquo;t exist, we return an empty feed:\nfunc parselocalfeed(path string) (*rss.feed, error) { b, err := os.readfile(path) if err != nil { if os.isnotexist(err) { return \u0026amp;rss.feed{}, nil } return \u0026amp;rss.feed{}, fmt.errorf(\u0026#34;unable to read local feed: %v\u0026#34;, err) } feed, err := parserssfeed(bytes.newbuffer(b)) if err != nil { return \u0026amp;rss.feed{}, fmt.errorf(\u0026#34;unable to parse local feed: %v\u0026#34;, err) } return feed, nil } next we parse the remote feed, and write the xml data to a file.\nfunc parseremotefeed(c *http.client, path, url string) (*rss.feed, error) { resp, err := c.get(url) if err != nil { return nil, fmt.errorf(\u0026#34;unable to get remote feed: %v\u0026#34;, err) } defer resp.body.close() data, err := io.readall(resp.body) if err != nil { return nil, err } if err := write(data, path); err != nil { return nil, err } feed, err := parserssfeed(bytes.newbuffer(data)) if err != nil { return nil, fmt.errorf(\u0026#34;unable to parse remote feed: %v\u0026#34;, err) } return feed, nil } we\u0026rsquo;re going to now create a getfeedupdates function which fetches the local feed, creates a map[string]time.time to lookup items fetches the remote feed, and call the lookupupdates function.\nfunc getfeedupdates(client *http.client, path, url string) ([]*rss.item, error) { localfeed, err := parselocalfeed(path) if err != nil { return nil, err } /* let\u0026#39;s create a map[string]time.time to quickly lookup items and compare dates */ itemmap := make(map[string]time.time) for _, item := range localfeed.items { formatted := item.pubdateparsed.format(time.rfc3339) pubdate, err := time.parse(time.rfc3339, formatted) if err != nil { return nil, fmt.errorf(\u0026#34;unable to parse date from local feed: %v\u0026#34;, err) } itemmap[item.title] = pubdate } // parse the remote copy of the opendata feed remotefeed, err := parseremotefeed(client, path, url) if err != nil { return nil, fmt.errorf(\u0026#34;unable to parse remote feed: %v\u0026#34;, err) } // make updateditems lists return lookupupdates(itemmap, remotefeed.items) } we need to lookup each item in the map we created earlier. if the item is not in the map, we add it to our updated items list. if the item is in the map, but the timestamps don\u0026rsquo;t match, we add it to updated items list.\nfunc lookupupdates(m map[string]time.time, items []*rss.item) ([]*rss.item, error) { updateditems := make([]*rss.item, 0) for _, i := range items { if date, ok := m[i.title]; ok { formatted := i.pubdateparsed.format(time.rfc3339) rdate, err := time.parse(time.rfc3339, formatted) if err != nil { return nil, err } if !rdate.equal(date) { updateditems = append(updateditems, i) } } else { updateditems = append(updateditems, i) } } return updateditems, nil } lasty, here are some helper functions to write to a local file and to parse the rss feed:\nfunc write(b []byte, path string) error { f, err := os.create(path) if err != nil { return err } if _, err := f.write(b); err != nil { return err } return nil } func parserssfeed(r io.reader) (*rss.feed, error) { fp := rss.parser{} feed, err := fp.parse(r) if err != nil { return nil, err } return feed, nil } when invoking this program, we\u0026rsquo;re going to pass it a flag identifying the path to the local xml and the remote url of the feed. we\u0026rsquo;re also going to have to add the citywindsor.ca ca cert. this is much easier than digging into:\ncurl: (60) ssl certificate problem: unable to get local issuer certificate our main function wraps all the above and outputs the changes in json format.\nfunc main() { flag.stringvar(\u0026amp;pathflag, \u0026#34;path\u0026#34;, \u0026#34;./feeds/opendata.xml\u0026#34;, \u0026#34;path to local xml file to diff\u0026#34;) flag.stringvar(\u0026amp;urlflag, \u0026#34;url\u0026#34;, \u0026#34;https://opendata.citywindsor.ca/rss\u0026#34;, \u0026#34;rss feed url\u0026#34;) flag.parse() /* note that we need to add the ca-cert for \u0026#34;citywindsor.ca\u0026#34; to to our http client in order to access the data programatically */ client, err := newclientwithca(cert) if err != nil { log.fatal(err) } // get our open data update list opendataupdates, err := getfeedupdates(client, pathflag, urlflag) if err != nil { log.fatal(err) } // exit if no changes found if len(opendataupdates) == 0 { log.printf(\u0026#34;no changes found\u0026#34;) } else { b, err := json.marshalindent(opendataupdates, \u0026#34;\u0026#34;, \u0026#34; \u0026#34;) if err != nil { log.fatal(err) } log.printf(\u0026#34;%s\u0026#34;, b) } } this code will likely change over time, but the concept works. next time, we\u0026rsquo;ll write the workflow that runs the rss-feed-aggregator, creates a new hugo post, updates the local copy of the feed, and commits the changes on a weekly basis.\nkeep coding with purpose! ::dev\n","title":"RSS Feed aggregation"},{"date":"2023-03-30","image":"","imageAlt":"","link":"https://dntiontk.github.io/posts/civic-code/","summary":"I frequently find myself on the City of Windsor website. I\u0026rsquo;ll scroll through city council meeting agendas and videos, browse the open data catalogue, check for newsroom headlines; annoyed by how outdated it feels the entire time.","tags":["analytics","city-council","civic-code","code","planning","politics","windsor"],"text":"i frequently find myself on the city of windsor website. i\u0026rsquo;ll scroll through city council meeting agendas and videos, browse the open data catalogue, check for newsroom headlines; annoyed by how outdated it feels the entire time. this isn\u0026rsquo;t about the look of the site either. i don\u0026rsquo;t really care about the asthetics. i\u0026rsquo;m talking about it feeling like the site is intentionally disjointed and built in a way to make engagement a little bit harder.\ni\u0026rsquo;m not saying that it is purposeful. but if it was, and the goal was to encourage apathy while maintaining plausible deniability, i would consider it a very effective website.\nthere are absolutely positive things about the website and there is a lot of data if don\u0026rsquo;t mind digging. however, the user experience is a deterrent.\nas you can probably tell from this site, my \u0026ldquo;user experience\u0026rdquo; skills aren\u0026rsquo;t the best\u0026hellip; but i do have the skills to improve my experience when interacting with the city\u0026rsquo;s data.\nthe plan i\u0026rsquo;ve broken the plan down into three areas of focus to start. there may be more changes, but this gives me a place to start.\nopen data there is an open data catalogue with 98 items, each of which have 1 or more data files. there is an rss feed, so we have the ability to be notified on updates. we can use this data to build out dashboards, maps and other forms of data visualization that are actually meaningful to our community. making data available is not the same as making data accessible.\ninfo feeds there are a few different rss feeds that the city maintains. the one that i\u0026rsquo;m particularly interested in is the newsroom headlines. pairing this with other local news rss feeds like cbc windsor would allow the building of a simple news aggregator. unfortunately, windsorite.ca does not have an rss feed, but maybe that is a future project.\ncouncil data this area was originally what got me fired up about this. why, in the year 2023, on soulja boy\u0026rsquo;s internet, do we not have downloadable transcriptions of city council meetings? you can get captions, if you watch the video on the janky sliq.net site, but its just the captions included in the screen recording of a zoom session. the meeting index is not well organized either. you have to open up a menu just to see what documents (agenda, minutes, video link, appendices, etc.) have been added to a particular meeting. the meeting documents, and the video are hosted on separate sites, meaning i need both open if i want to follow along. also, i shouldn\u0026rsquo;t need a browser to download city council meeting videos. ideally, i should be able to download the video, transcription and associated documents via an api.\nthe action the goal is to create a bunch of smaller programs (mostly in golang, but maybe some python if necessary) that can interact with this blog in an automated way to give me (and hopefully others) a more user-friendly experience. this also allows me to play with certain disciplines (ai/machine learning, rss feed aggregation, data vizualization) in a low-stakes sort of way.\nkeep coding with purpose! ::dev\n","title":"civic code"},{"date":"0001-01-01","image":"","imageAlt":"","link":"https://dntiontk.github.io/about/","summary":"I\u0026rsquo;m a Black software developer based in Windsor, Ontario. I like to write automation and I care about local politics. I hope this blog can act as a bridge between these interests in a benefitial way.","tags":[],"text":"i\u0026rsquo;m a black software developer based in windsor, ontario. i like to write automation and i care about local politics. i hope this blog can act as a bridge between these interests in a benefitial way. thanks for visiting.\nkeep coding with purpose! ::dev\n","title":""}]
}

